{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brettfazio/AI-Homework/blob/master/HW_5/Homework5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2s6mHbhc8KjL",
        "colab_type": "text"
      },
      "source": [
        "### Homework 5\n",
        "\n",
        "**Problem 1**\n",
        "\n",
        "Summarize and describe the different concepts/methods/algorithms that you have learned in this course.\n",
        "\n",
        "Use a Colab notebook. Make sure that you organize the material logically by using sections/subsections. Also, use code cell to include code snippets.\n",
        "\n",
        "I suggest that you group everything into five categories:\n",
        "\n",
        "1. General concepts (for instance, what is artificial intelligence, machine learning, deep learning)\n",
        "\n",
        "2. Basic concepts (for instance, here you can talk about linear regression, logistic regression, gradients, gradient descent)\n",
        "\n",
        "3. Building a model (for instance, here you can talk about the structure of a convent, what it components are etc.)\n",
        "\n",
        "4. Comping a model (for instance, you can talk here about optimizers, learning rate etc.)\n",
        "\n",
        "5. Training a model (for instance, you can talk about overfitting/underfitting)\n",
        "\n",
        "6. Finetuning  a pretrained model (describe how you proceed)\n",
        "\n",
        "Take this homework *very seriously*.  You have the opportunity to make up for lost point on previous homework assignments.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiZ4xCD3M1tO",
        "colab_type": "text"
      },
      "source": [
        "Some quotations taken from https://github.com/schneider128k/machine_learning_course"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROLpnvkQPqcr",
        "colab_type": "text"
      },
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3nui_DOPtS1",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UuFcdSj8OeB",
        "colab_type": "text"
      },
      "source": [
        "## Section 1 General Concepts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIFFvZJ58h15",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "### Artificial Intelligence \n",
        "  - \"science and engineering of making intelligent machines\" - John McCarthy\n",
        "  - Symbolic AI - methods in AI research that are human readable aka \"symbolic.\" GOFAI falls under this.\n",
        "  - **Input & Rules produce output.**\n",
        "\n",
        "### Machine Learning \n",
        "  - \"field of study that gives computers the ability to learn without explicitly being programmed\" - Arthur Samuel\n",
        "  - \"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E\" - Tom Mitchell.\n",
        "  - **Input & output produce rules.**\n",
        "\n",
        "### Deep Learning \n",
        "  - Subset of ML, utilizes networks that are able to learn about data in an unsupervised manor. \n",
        "  - Neural networks are an example of this.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOEmLIuk8RsL",
        "colab_type": "text"
      },
      "source": [
        "## Section 2 Basic Concepts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmpdZ40t8izQ",
        "colab_type": "text"
      },
      "source": [
        "At a basic level regression predicts discrete data (yes or no, 1 or 2 or 3, etc.).\n",
        "\n",
        "## Linear Regression\n",
        "\n",
        "Linear Regression - A linear approach to modeling between one or more variables.\n",
        "\n",
        "At a basic level your linear regression function could look like this:\n",
        "\n",
        "$$ \\hat y = b + w_1 x_1 $$\n",
        "\n",
        "In the above $ \\hat y $ is the output you are trying to predict. $ b $ is your bias, $ x_1 $ is your feature, and $ w_1 $ is your weight on $ x_1 $.\n",
        "\n",
        "We could make this more complicated for a few variable by just expanding the function like so:\n",
        "\n",
        "$$ \\hat y = b + w_1 x_1 + w_2 x_2 + ... + w_n x_n $$\n",
        "\n",
        "Where the above would have $ n $ features.\n",
        "\n",
        "## Logistic Regression\n",
        "\n",
        "Logistic Regression is used to predict a value between 0 and 1. As such it is often used for classification problems as we could have 0 repesent \"dog\" and 1 represent \"cat.\" In that example, the closer it is to 0 the more likely it is a dog (according to the model) and the closer to 1 the more likely it is a cat.\n",
        "\n",
        "Logistic regression uses the *sigmoid function* to map these values between 0 and 1.\n",
        "\n",
        "$$ \\sigma (z) = \\frac{1}{1 + e^{-z}}$$\n",
        "\n",
        "## Gradients\n",
        "\n",
        "Gradients are the steepness of the slope at a certain point.\n",
        "\n",
        "A gradient is defined as so:\n",
        "\n",
        "$$ \\triangledown f(p) = [ \\frac{\\delta f}{\\delta x_1}(p) ...  \\frac{\\delta f}{\\delta x_n}(p)] $$\n",
        "\n",
        "We use gradients to make our \"steps\" when performing gradient descent, as gradients have both magnitude and direction.\n",
        "\n",
        "## Gradient Descent\n",
        "\n",
        "Gradient Descent can be thought of as an iterative trial and error process. In which first a starting value is chosen, and then the gradient is used to make steps that (hopefully) lead to a good value. (I say hopefully as gradient descent needs to be tuned and there's a possibility one could jump too far and totally miss the answer).\n",
        "\n",
        "Once an initial value is chosen the gradient, as described above, is used to make steps.\n",
        "\n",
        "To be percise, if we are stepping on $ w $ we make the steps like so:\n",
        "\n",
        "$$ w = w - \\alpha \\triangledown  L$$\n",
        "\n",
        "In the above $ \\alpha $ is the learning rate that determines how big we are going to step. The learning rate is considered a *hyperparameter*  (\"a parameter external to the model\").\n",
        "\n",
        "We want $ \\alpha $ to be just the right size so we can efficiently get to our answer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqGgS1wgGC3I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHum3GThF-qR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Implementing the sigmoid function using numpy\n",
        "\n",
        "\n",
        "def sigmoid(z):\n",
        "  return 1 / (1 + np.exp(-1*z))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uRhsg6e8Uhz",
        "colab_type": "text"
      },
      "source": [
        "## Section 3 Building a Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zs-loQl8jpt",
        "colab_type": "text"
      },
      "source": [
        "### Overview\n",
        "\n",
        "A basic neural network typically consists of 3 layers: input, hidden, output.\n",
        "\n",
        "#### Input\n",
        "\n",
        "The input layer is the layer that accepts the input data (i.e. the data that we are trying to learn on). If were accepting the MINST dataset our input layer could look like this:\n",
        "\n",
        "```\n",
        "network.add(tf.keras.layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
        "```\n",
        "\n",
        "With the input shape being `28*28` because the MINST images are `28*28`.\n",
        "\n",
        "#### Hidden\n",
        "\n",
        "The hidden layers are the layers in the middle of our model (all the layers that are not the input or output). These layers help to add complexity to our model to extract deeper insights.\n",
        "\n",
        "Adding too many hidden layers can possibly make the model less accurate.\n",
        "\n",
        "#### Output\n",
        "\n",
        "The output layer is the layer that gives us the resulting information we want. Often times we want to select a specific activation function based on the type of problem we are presented with. With hte MNIST example again, here is an example output layer:\n",
        "\n",
        "```\n",
        "network.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "```\n",
        "\n",
        "In the above, the `softmax` activation function is used.\n",
        "\n",
        "### ConvNets\n",
        "\n",
        "A convnet is a subclass of neural networks often used for images because of how it can learn to extract higher level features from an image.\n",
        "\n",
        "### Structure of a convnet\n",
        "\n",
        "Convnets usually consist of a series of \"modules\" in which `conv2d`, `ReLU`, and `maxpool` operations are performed. There are performed for feature extraction purposes.\n",
        "\n",
        "After the \"modules\" there is a classification layer. This is a series of dense layers that turns the information into some sort of classification (this will of course depend on the problem at hand).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqgD3-XW8XMG",
        "colab_type": "text"
      },
      "source": [
        "## Section 4 Compiling a Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQ9VU6Kt8kS5",
        "colab_type": "text"
      },
      "source": [
        "### Loss Function\n",
        "\n",
        "The loss function is the quantity that will be minimized during training. In the example below, `categorical_crossentropy` is the used loss function. `categorical_crossentropy` is the loss function commonly used in multi-label classification problems. If one wanted to do binary classification, for example, a common choice is `binary_crossentropy`.\n",
        "\n",
        "MSE (Mean Squared Error) is also a popular loss function.\n",
        "\n",
        "It is defined as follows:\n",
        "\n",
        "$$\n",
        "L_{SE} = \\frac{1}{2} (\\alpha-y)^2\n",
        "$$\n",
        "\n",
        "### Optimizers\n",
        "\n",
        "The optimizer determines how the model will be updated based on the loss function. For example, in the implementation section below you can see that the optimizer used is `rmsprop`. \n",
        "\n",
        "There are many different optimizers you can use in keras as listed [here](https://keras.io/optimizers/).\n",
        "\n",
        "### Learning Rate\n",
        "\n",
        "The learning rate, sometimes denoted $\\alpha$, is a *hyperparameter* (external to the model). It controls how quickly the model learns. If it is too small the model will take forever to fit, if it is too large it is possible it could jump the minimum and produce incorrect or inaccurate results.\n",
        "\n",
        "### Implementation\n",
        "\n",
        "If we build a model using `tf.keras.models.Sequential()` called `model` we can compile it like so using the `compile` function:\n",
        "\n",
        "```\n",
        "network.compile(optimizer='rmsprop',\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0w7JnYeGIWY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Implemeting binary cross entropy in numpy\n",
        "\n",
        "def binary_cross_entropy(y, a):\n",
        "  return -y * np.log10(a) - (1-y) * np.log10(1-a)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcm93zWqopwZ",
        "colab_type": "text"
      },
      "source": [
        "Here is an example of function that computes loss using the `binary_cross_entropy` and `sigmoid` functions I defined earlier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXvlm0pLolfI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_loss(A, B):\n",
        "  loss = 0\n",
        "  for data_val, label in zip(A, B):\n",
        "    pred = np.dot(np.reshape(manual_weights, (2,)), data_val) + manual_bias\n",
        "    bce = binary_cross_entropy(label, sigmoid(pred))\n",
        "    loss += bce\n",
        "  loss /= (partial)\n",
        "  return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7lKTblZ8Z34",
        "colab_type": "text"
      },
      "source": [
        "## Section 5 Training a Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqnwa6sk8k6d",
        "colab_type": "text"
      },
      "source": [
        "### Training\n",
        "\n",
        "At a high level training means iteratively adjusting the weights and the bias in an attempt to minimize the loss and thus increase accuracy of the model.\n",
        "\n",
        "### Loss\n",
        "\n",
        "At a high level loss is simply just the penalty for making an incorrect prediction. A perfect prediction would be 0, anything else will be higher than 0. I gave an example of computing loss above with `compute_loss`, `binary_cross_entropy`, and `sigmoid`.\n",
        "\n",
        "In combining that definition of training and loss we get a process called **empiricial risk minimization** (source: [here](https://github.com/schneider128k/machine_learning_course/blob/master/slides/2_c_slides.pdf)).\n",
        "\n",
        "### Implementation\n",
        "\n",
        "To train a model using Keras/Tensorflow you use the `.fit` function.\n",
        "\n",
        "From the provided [MNIST example notebook](https://colab.research.google.com/drive/144nj1SRtSjpIcKZgH6-GPdA9bWkg68nh):\n",
        "\n",
        "We built and complied a model `network` and then on network we will call `.fit`:\n",
        "\n",
        "```\n",
        "epochs = 10\n",
        "history = network.fit(train_images, \n",
        "                      train_labels, \n",
        "                      epochs=epochs, \n",
        "                      batch_size=128, \n",
        "                      validation_data=(test_images, test_labels))\n",
        "```\n",
        "\n",
        "The `.fit` call will train our model with the provided training set, number of epochs, desired batch size, and a validation set.\n",
        "\n",
        "\n",
        "### Potential Problems\n",
        "\n",
        "\n",
        "#### Overfitting\n",
        "\n",
        "Overfitting is the problem that arises when your model is too complex for the provided task and it skews the results in an incorrect manner.\n",
        "\n",
        "Often times what will happen is the model will become too accruate on the test data that when run on any other data set (like a validation set) it will be incredibility inaccurate.\n",
        "\n",
        "There are many ways to fix overfitting. One way is to use a model more suited for the task at hand, if your model is too complicated for the task it may be best to use a simplier model. You may also want to modify your training data, possibly removing outliers and getting rid of meaningless features.\n",
        "\n",
        "#### Underfitting\n",
        "\n",
        "Underfitting is the opposite problem of overfitting. Where overfitting is the model being too complex and getting too accurate on the training data - underfitting is when the model is not complex enough. Thus since the model is not complex enough, the accuracy will not be high on either the validation or test set.\n",
        "\n",
        "A simple way to fix this is to increase the complexity of your model, or possibly use a complex pre-made model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-bHkqjjEiYJ",
        "colab_type": "text"
      },
      "source": [
        "Source/idea for the code [here](https://colab.research.google.com/drive/144nj1SRtSjpIcKZgH6-GPdA9bWkg68nh#scrollTo=mHp0sz7cYPsK&line=1&uniqifier=1)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cY7ZRRJlEZwx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Putting building, compiling, and training together to do MNIST learning.\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "\n",
        "# Set up data\n",
        "\n",
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "train_data, test_data = mnist.load_data()\n",
        "\n",
        "train_images_original, train_labels_original = train_data\n",
        "test_images_original, test_labels_original = test_data\n",
        "\n",
        "train_images = train_images_original.reshape((60000, 28 * 28))\n",
        "train_images = train_images.astype('float32') / 255\n",
        "\n",
        "test_images = test_images_original.reshape((10000, 28 * 28))\n",
        "test_images = test_images.astype('float32') / 255\n",
        "\n",
        "# Label data\n",
        "\n",
        "train_labels = tf.keras.utils.to_categorical(train_labels_original)\n",
        "test_labels = tf.keras.utils.to_categorical(test_labels_original)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHlihyOZEtgx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# BUILD\n",
        "\n",
        "# define sequential model\n",
        "network = tf.keras.models.Sequential()\n",
        "# add two layers to model\n",
        "network.add(tf.keras.layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
        "network.add(tf.keras.layers.Dense(10, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKmM3BfFExDN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# COMPILE\n",
        "\n",
        "network.compile(optimizer='rmsprop',\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSoowefaEzSK",
        "colab_type": "code",
        "outputId": "e69c7cff-5259-48d9-9cf6-06f0878155dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "# TRAIN\n",
        "\n",
        "epochs = 5\n",
        "history = network.fit(train_images, \n",
        "                      train_labels, \n",
        "                      epochs=epochs, \n",
        "                      batch_size=128, \n",
        "                      validation_data=(test_images, test_labels))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "469/469 [==============================] - 5s 10ms/step - loss: 0.2550 - accuracy: 0.9257 - val_loss: 0.1267 - val_accuracy: 0.9620\n",
            "Epoch 2/5\n",
            "469/469 [==============================] - 5s 10ms/step - loss: 0.1033 - accuracy: 0.9699 - val_loss: 0.0827 - val_accuracy: 0.9747\n",
            "Epoch 3/5\n",
            "469/469 [==============================] - 5s 10ms/step - loss: 0.0674 - accuracy: 0.9797 - val_loss: 0.0766 - val_accuracy: 0.9771\n",
            "Epoch 4/5\n",
            "469/469 [==============================] - 5s 10ms/step - loss: 0.0489 - accuracy: 0.9855 - val_loss: 0.0643 - val_accuracy: 0.9798\n",
            "Epoch 5/5\n",
            "469/469 [==============================] - 5s 10ms/step - loss: 0.0359 - accuracy: 0.9894 - val_loss: 0.0717 - val_accuracy: 0.9783\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZUru8vk8b3Q",
        "colab_type": "text"
      },
      "source": [
        "## Section 6 Finetuning a Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtbZZ8vV8ls2",
        "colab_type": "text"
      },
      "source": [
        "#### Fine-tuning is the process one can undertake when using a pretrained model (such as [these](https://keras.io/applications/)) to increase its accuracy and useability on your specific data set / use case. We are trying to get the model to work well on our specific data set.\n",
        "\n",
        "If you look at the keras applications there are lots of pretrained models such sa Xception, VGG16, and VGG19. On these models we can fine-tune them to work on our specific data set.\n",
        "\n",
        "*Using the provided notes from class ([here](https://colab.research.google.com/drive/1uVLIUWdT7--b59vM7NaSHkB-qFcu30jU#scrollTo=dI5rmt4UBwXs)) and my homework 4 I will give some specific examples.*\n",
        "\n",
        "There are two popular ways to fine tune a model.\n",
        "\n",
        "------------\n",
        "\n",
        "### First is to add additional layers on the model.\n",
        "\n",
        "Looking at my [homework 4 problem 3](https://colab.research.google.com/drive/1O1hvDELqlPM3Ybg7LYfHcS2M73zI-FPg):\n",
        "\n",
        "\n",
        "```\n",
        "from keras.applications import Xception\n",
        "\n",
        "conv_base = Xception(\n",
        "    weights='imagenet', \n",
        "    include_top=False, \n",
        "    input_shape=(150, 150, 3))\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(256, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "```\n",
        "\n",
        "You can see I used a pretrained model from keras, in this situation it was `Xception`.\n",
        "\n",
        "When I create my sequential model, I add my `Xception` model but then I add a `relu` activation layer and a `sigmoid` activation layer. \n",
        "\n",
        "While this does not directly change the `conv_base` pretrained model it does help to improve our performance as we can add layers specific to our use case - whether we're doing binary classification or a different classification problem.\n",
        "\n",
        "-------------\n",
        "\n",
        "### Second is to fine tune the premade model itself.\n",
        "\n",
        "We can actually change the weights on the top layers of the pretrained model by \"unfreezing\" certain layers after a layer in a model.\n",
        "\n",
        "From my homework 4 again:\n",
        "\n",
        "```\n",
        "conv_base.trainable = True\n",
        "\n",
        "set_trainable = False\n",
        "for layer in conv_base.layers:\n",
        "  if layer.name == 'conv2d_3':\n",
        "    set_trainable = True\n",
        "  if set_trainable:\n",
        "    layer.trainable = True\n",
        "  else:\n",
        "    layer.trainable = False\n",
        "```\n",
        "\n",
        "You can see that I set all layers after `conv2d_3` to trainable. That means when I run my pretrained model those layers that I set to trainable will actually adjust their weights.\n",
        "\n",
        "From the provided in class notes that I refereced earlier there are two important notes about this:\n",
        "  1. \"Fine-tuning should only be attempted after you have trained the top-level classifier with the pretrained model set to non-trainable.\"\n",
        "  2. \"We fine-tune only the top layers of the pre-trained model rather than all layers of the pretrained model.\"\n",
        "\n",
        "Keeping those concepts in mind, when fine tuning a pretrained model you should see a performance improvement as that is the goal. If performance degradation occurs then it is possible that layers were unfrozen too high up in the pretrained model messing with the original performance of the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GanHKRBpE6H9",
        "colab_type": "code",
        "outputId": "65dc252f-46f8-4e88-bcdc-4b45a4a8e075",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "# Building a model from a pre-existing model and then adding extra layers to fine tune it.\n",
        "\n",
        "from keras import layers\n",
        "from keras import models\n",
        "from keras import optimizers\n",
        "\n",
        "\n",
        "from keras.applications import Xception\n",
        "\n",
        "conv_base = Xception(\n",
        "    weights='imagenet', \n",
        "    include_top=False, \n",
        "    input_shape=(150, 150, 3))\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(layers.Flatten())\n",
        "# Extra layers to fine tune.\n",
        "model.add(layers.Dense(256, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "xception (Model)             (None, 5, 5, 2048)        20861480  \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 51200)             0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 256)               13107456  \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 33,969,193\n",
            "Trainable params: 33,914,665\n",
            "Non-trainable params: 54,528\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}